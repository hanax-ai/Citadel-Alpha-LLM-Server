# Basic performance verification
echo "=== vLLM Performance Verification ==="
source /opt/citadel/dev-env/bin/activate

python << 'EOF'
import torch
import time
from vllm import LLM, SamplingParams

if torch.cuda.is_available():
    print("Testing vLLM performance...")
    
    # Use a small model for performance testing
    try:
        llm = LLM(
            model="facebook/opt-125m",
            tensor_parallel_size=1,
            gpu_memory_utilization=0.3
        )
        
        # Performance test
        prompts = ["Hello world!"] * 10
        sampling_params = SamplingParams(max_tokens=20)
        
        start_time = time.time()
        outputs = llm.generate(prompts, sampling_params)
        end_time = time.time()
        
        total_time = end_time - start_time
        throughput = len(prompts) / total_time
        
        print(f"✅ Performance test completed:")
        print(f"   Requests: {len(prompts)}")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   Throughput: {throughput:.2f} requests/second")
        
    except Exception as e:
        print(f"❌ Performance test failed: {e}")
else:
    print("❌ CUDA not available for performance testing")
EOF