# PLANB-05: Latest vLLM Installation with Compatibility Resolution

**Task:** Install latest vLLM version with proper PyTorch compatibility for Python 3.12  
**Duration:** 60-90 minutes  
**Prerequisites:** PLANB-01 through PLANB-04 completed, Python 3.12 and PyTorch installed  

## Overview

This task installs the latest compatible vLLM version, resolving the PyTorch compatibility issues encountered in the original installation. We'll use vLLM 0.6.x series which is compatible with PyTorch 2.2+ and Python 3.12.

## vLLM Compatibility Matrix

### Target Configuration
- **vLLM Version**: 0.6.1+ (latest stable)
- **PyTorch Version**: 2.4+ (already installed)
- **CUDA Version**: 12.4+ (already installed)
- **Python Version**: 3.12 (already installed)
- **GPU Architecture**: Ada Lovelace (RTX 4070 Ti SUPER)

### Compatibility Resolution
```yaml
compatibility_matrix:
  vllm_0.6.1:
    pytorch: ">=2.1.0,<2.8.0"
    python: ">=3.8,<=3.12"
    cuda: ">=11.8,<=12.5"
    transformers: ">=4.36.0"
    status: "‚úÖ COMPATIBLE"
  
  vllm_0.2.7:
    pytorch: "==2.1.2"
    python: ">=3.8,<=3.11"
    cuda: ">=11.8,<=12.2"
    transformers: ">=4.28.0"
    status: "‚ùå INCOMPATIBLE (PyTorch 2.1.2 unavailable)"
```

## Pre-Installation Steps

### Step 1: Environment Preparation

1. **Verify Current Installation State**
   ```bash
   # Activate development environment
   source /opt/citadel/dev-env/bin/activate
   
   # Check current Python and PyTorch versions
   echo "=== Current Environment State ==="
   python --version
   python -c "import torch; print(f'PyTorch: {torch.__version__}')"
   python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
   python -c "import torch; print(f'CUDA Version: {torch.version.cuda}')"
   pip list | grep -E "(torch|transformers|vllm)"
   ```

2. **Clean Previous vLLM Installation**
   ```bash
   # Remove any existing vLLM installation
   pip uninstall vllm -y
   pip uninstall vllm-flash-attn -y
   pip uninstall flash-attn -y
   
   # Clear pip cache
   pip cache purge
   
   # Verify clean state
   pip list | grep vllm || echo "vLLM successfully removed"
   ```

3. **Update Dependencies**
   ```bash
   # Update core dependencies to latest compatible versions
   pip install --upgrade \
     pip \
     setuptools \
     wheel \
     packaging \
     ninja
   
   # Update PyTorch to latest stable
   pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
   
   # Verify PyTorch update
   python -c "import torch; print(f'Updated PyTorch: {torch.__version__}')"
   ```

### Step 2: System Dependencies Installation

1. **Install Build Dependencies**
   ```bash
   # Install system packages required for vLLM compilation
   sudo apt update
   sudo apt install -y \
     build-essential \
     cmake \
     ninja-build \
     python3.12-dev \
     libopenmpi-dev \
     libaio-dev \
     libcurl4-openssl-dev \
     libssl-dev \
     libffi-dev \
     libnuma-dev \
     pkg-config
   
   # Install additional compilation tools
   sudo apt install -y \
     gcc-11 \
     g++-11 \
     libc6-dev \
     libc-dev-bin \
     linux-libc-dev
   
   # Set GCC version for compilation
   sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100
   sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 100
   ```

2. **Configure Compilation Environment**
   ```bash
   # Set compilation environment variables
   export CC=gcc-11
   export CXX=g++-11
   export CUDA_HOME=/usr/local/cuda
   export NVCC_PREPEND_FLAGS='-ccbin /usr/bin/gcc-11'
   export TORCH_CUDA_ARCH_LIST="8.9"  # For RTX 4070 Ti SUPER
   export MAX_JOBS=8  # Limit parallel compilation to prevent OOM
   
   # Add to shell profile for persistence
   tee -a ~/.bashrc << 'EOF'
   
   # vLLM Compilation Environment
   export CC=gcc-11
   export CXX=g++-11
   export NVCC_PREPEND_FLAGS='-ccbin /usr/bin/gcc-11'
   export TORCH_CUDA_ARCH_LIST="8.9"
   export MAX_JOBS=8
   EOF
   
   source ~/.bashrc
   ```

## vLLM Installation Methods

Choose one of the following installation approaches based on your preference and requirements:

### Method 1: Quick Install (Recommended)
**Duration**: 15-30 minutes
**Complexity**: Low
**Best for**: Users who want a fast, proven installation with minimal steps

This method uses a proven command sequence that successfully installs vLLM and all required dependencies in a single execution.

#### Prerequisites
- PLANB-01 through PLANB-04 completed
- Python 3.12 and PyTorch installed
- `/opt/citadel/dev-env` virtual environment available

#### Installation Command
```bash
# Activate environment and install vLLM with all dependencies
source /opt/citadel/dev-env/bin/activate && \
pip install vllm && \
pip install \
    transformers>=4.36.0 \
    tokenizers>=0.15.0 \
    accelerate>=0.25.0 \
    bitsandbytes>=0.41.0 \
    scipy>=1.11.0 \
    numpy>=1.24.0 \
    requests>=2.31.0 \
    aiohttp>=3.9.0 \
    fastapi>=0.104.0 \
    uvicorn>=0.24.0 \
    pydantic>=2.5.0 \
    huggingface-hub>=0.19.0 && \
pip install \
    prometheus-client>=0.19.0 \
    psutil>=5.9.0 \
    GPUtil>=1.4.0 \
    py3nvml>=0.2.7

# Verify installation
echo "=== Installation Verification ==="
python -c "import vllm; print(f'‚úÖ vLLM version: {vllm.__version__}')"
python -c "import torch; print(f'‚úÖ PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'‚úÖ Transformers: {transformers.__version__}')"
python -c "import torch; print(f'‚úÖ CUDA available: {torch.cuda.is_available()}')"
echo "üéâ Quick installation completed successfully!"
```

#### Installation Notes
- **Proven Success**: This exact command sequence has been tested and verified to work
- **Latest vLLM**: Installs the current stable version (not the problematic 0.2.7)
- **Complete Dependencies**: All required packages installed with compatible versions
- **Single Execution**: Entire installation completes without user intervention
- **Built-in Verification**: Immediate confirmation of successful installation

#### Troubleshooting Quick Install
If the quick install fails:
1. **Check Environment**: Ensure `/opt/citadel/dev-env` exists and is activated
2. **Clear Cache**: Run `pip cache purge` and retry
3. **Check Dependencies**: Verify Python 3.12 and PyTorch are installed
4. **Use Detailed Method**: Fall back to Method 2 for step-by-step debugging

---

### Method 2: Detailed Install (Comprehensive)
**Duration**: 60-90 minutes
**Complexity**: High
**Best for**: Users who need step-by-step control and debugging capabilities

#### Step 1: Install Latest vLLM

1. **Install vLLM from PyPI**
   ```bash
   # Activate development environment
   source /opt/citadel/dev-env/bin/activate
   
   # Install latest vLLM version
   echo "Installing vLLM latest version..."
   pip install vllm
   
   # Check installed version
   python -c "import vllm; print(f'vLLM version: {vllm.__version__}')"
   ```

2. **Alternative: Install from Source (if PyPI fails)**
   ```bash
   # If PyPI installation fails, install from source
   echo "Installing vLLM from source..."
   
   # Clone vLLM repository
   cd /tmp
   git clone https://github.com/vllm-project/vllm.git
   cd vllm
   
   # Checkout latest stable tag
   git checkout $(git describe --tags --abbrev=0)
   
   # Install in development mode
   pip install -e .
   
   # Verify installation
   python -c "import vllm; print(f'vLLM version: {vllm.__version__}')"
   
   # Clean up
   cd /opt/citadel
   rm -rf /tmp/vllm
   ```

### Step 2: Install vLLM Dependencies

1. **Install Core Dependencies**
   ```bash
   # Install core vLLM dependencies with specific versions
   pip install \
     transformers>=4.36.0 \
     tokenizers>=0.15.0 \
     sentencepiece>=0.1.99 \
     numpy>=1.24.0 \
     requests>=2.31.0 \
     aiohttp>=3.9.0 \
     pydantic>=2.5.0 \
     pydantic-core>=2.14.0 \
     typing-extensions>=4.8.0
   
   # Install additional ML dependencies
   pip install \
     accelerate>=0.25.0 \
     scipy>=1.11.0 \
     scikit-learn>=1.3.0 \
     datasets>=2.14.0 \
     evaluate>=0.4.0 \
     safetensors>=0.4.0
   ```

2. **Install Web Framework Dependencies**
   ```bash
   # Install FastAPI and related packages for API server
   pip install \
     fastapi>=0.104.0 \
     uvicorn[standard]>=0.24.0 \
     python-multipart>=0.0.6 \
     httpx>=0.25.0 \
     aiofiles>=23.2.1 \
     jinja2>=3.1.2
   
   # Install additional server dependencies
   pip install \
     gunicorn>=21.2.0 \
     prometheus-client>=0.19.0 \
     opentelemetry-api>=1.21.0 \
     opentelemetry-sdk>=1.21.0
   ```

3. **Install Monitoring and Utilities**
   ```bash
   # Install monitoring and system utilities
   pip install \
     psutil>=5.9.0 \
     GPUtil>=1.4.0 \
     py3nvml>=0.2.7 \
     nvidia-ml-py3>=7.352.0 \
     rich>=13.7.0 \
     typer>=0.9.0 \
     tqdm>=4.66.0
   
   # Install development and debugging tools
   pip install \
     ipython>=8.17.0 \
     jupyter>=1.0.0 \
     matplotlib>=3.7.0 \
     seaborn>=0.12.0 \
     tensorboard>=2.15.0
   ```

### Step 3: Flash Attention Installation (Optional)

1. **Install Flash Attention for Performance**
   ```bash
   # Flash Attention significantly improves performance for long sequences
   echo "Installing Flash Attention..."
   
   # Install flash-attn (this may take 10-15 minutes to compile)
   pip install flash-attn --no-build-isolation
   
   # Verify installation
   python -c "import flash_attn; print('Flash Attention installed successfully')" || echo "Flash Attention installation failed (optional)"
   ```

2. **Alternative Flash Attention Installation**
   ```bash
   # If standard installation fails, try with specific options
   pip install flash-attn --no-build-isolation --no-cache-dir
   
   # Or install pre-compiled wheel if available
   pip install flash-attn --find-links https://github.com/Dao-AILab/flash-attention/releases
   ```

### Step 4: Hugging Face Integration

1. **Install Hugging Face CLI and Configure Authentication**
   ```bash
   # Install Hugging Face Hub CLI
   pip install huggingface_hub[cli]>=0.19.0
   
   # Configure Hugging Face authentication with token
   echo "Configuring Hugging Face authentication..."
   echo "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" | huggingface-cli login --token
   
   # Set environment variables for session
   export HF_TOKEN=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
   export HUGGINGFACE_HUB_TOKEN=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
   export HF_HOME=/mnt/citadel-models/cache
   export TRANSFORMERS_CACHE=/mnt/citadel-models/cache/transformers
   
   # Verify authentication
   huggingface-cli whoami
   
   echo "‚úÖ Hugging Face authentication configured successfully"
   ```

## vLLM Configuration and Testing

### Step 1: Create vLLM Test Script

1. **Create Basic vLLM Test**
   ```bash
   # Create vLLM functionality test
   tee /opt/citadel/scripts/test-vllm.py << 'EOF'
   #!/usr/bin/env python3
   """
   Basic vLLM functionality test
   """
   
   import os
   import sys
   import time
   import torch
   from rich.console import Console
   from rich.progress import Progress
   
   console = Console()
   
   def test_vllm_import():
       """Test vLLM import and basic functionality"""
       try:
           import vllm
           console.print(f"‚úÖ vLLM imported successfully: {vllm.__version__}")
           return True
       except ImportError as e:
           console.print(f"‚ùå vLLM import failed: {e}")
           return False
   
   def test_cuda_availability():
       """Test CUDA availability"""
       if torch.cuda.is_available():
           console.print(f"‚úÖ CUDA available: {torch.version.cuda}")
           console.print(f"‚úÖ GPU count: {torch.cuda.device_count()}")
           for i in range(torch.cuda.device_count()):
               console.print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
           return True
       else:
           console.print("‚ùå CUDA not available")
           return False
   
   def test_vllm_engine():
       """Test vLLM engine initialization with a small model"""
       try:
           from vllm import LLM, SamplingParams
           
           console.print("üß™ Testing vLLM engine with small model...")
           
           # Use a small model for testing
           model_name = "facebook/opt-125m"
           
           # Initialize LLM
           llm = LLM(
               model=model_name,
               tensor_parallel_size=1,
               gpu_memory_utilization=0.3,
               download_dir="/tmp/vllm_test_cache"
           )
           
           # Test generation
           prompts = ["Hello, how are you?"]
           sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=50)
           
           outputs = llm.generate(prompts, sampling_params)
           
           for output in outputs:
               prompt = output.prompt
               generated_text = output.outputs[0].text
               console.print(f"‚úÖ Test generation successful:")
               console.print(f"  Prompt: {prompt}")
               console.print(f"  Generated: {generated_text}")
           
           return True
           
       except Exception as e:
           console.print(f"‚ùå vLLM engine test failed: {e}")
           return False
   
   def main():
       console.print("üöÄ vLLM Installation Test Suite")
       console.print("=" * 50)
       
       tests = [
           ("vLLM Import", test_vllm_import),
           ("CUDA Availability", test_cuda_availability),
           ("vLLM Engine", test_vllm_engine)
       ]
       
       results = []
       for test_name, test_func in tests:
           console.print(f"\nüìã Running {test_name} test...")
           result = test_func()
           results.append((test_name, result))
       
       console.print(f"\nüìä Test Results Summary:")
       console.print("-" * 30)
       
       passed = 0
       for test_name, result in results:
           status = "‚úÖ PASSED" if result else "‚ùå FAILED"
           console.print(f"  {test_name}: {status}")
           if result:
               passed += 1
       
       console.print(f"\nOverall: {passed}/{len(tests)} tests passed")
       
       if passed == len(tests):
           console.print("üéâ All tests passed! vLLM is ready for use.")
           return 0
       else:
           console.print("‚ö†Ô∏è Some tests failed. Check installation.")
           return 1
   
   if __name__ == "__main__":
       sys.exit(main())
   EOF
   
   chmod +x /opt/citadel/scripts/test-vllm.py
   ```

2. **Run vLLM Test**
   ```bash
   # Activate environment and run test
   source /opt/citadel/dev-env/bin/activate
   python /opt/citadel/scripts/test-vllm.py
   ```

### Step 2: Create vLLM Service Scripts

1. **Create Simple vLLM Server Script**
   ```bash
   # Create basic vLLM server script
   tee /opt/citadel/scripts/start-vllm-server.py << 'EOF'
   #!/usr/bin/env python3
   """
   Simple vLLM server startup script for testing
   """
   
   import argparse
   import subprocess
   import sys
   import time
   from pathlib import Path
   
   def start_vllm_server(model_path, port=8000, host="0.0.0.0"):
       """Start vLLM OpenAI-compatible API server"""
       
       if not Path(model_path).exists():
           print(f"‚ùå Model path not found: {model_path}")
           return False
       
       cmd = [
           "python", "-m", "vllm.entrypoints.openai.api_server",
           "--model", model_path,
           "--host", host,
           "--port", str(port),
           "--tensor-parallel-size", "1",
           "--gpu-memory-utilization", "0.7",
           "--trust-remote-code"
       ]
       
       print(f"üöÄ Starting vLLM server...")
       print(f"   Model: {model_path}")
       print(f"   Host: {host}")
       print(f"   Port: {port}")
       print(f"   Command: {' '.join(cmd)}")
       
       try:
           process = subprocess.Popen(cmd)
           print(f"‚úÖ Server started with PID: {process.pid}")
           print(f"üåê API will be available at: http://{host}:{port}")
           print("üìö API docs at: http://localhost:8000/docs")
           
           # Wait for process
           process.wait()
           
       except KeyboardInterrupt:
           print("\nüõë Server stopped by user")
           process.terminate()
       except Exception as e:
           print(f"‚ùå Server failed: {e}")
           return False
       
       return True
   
   def main():
       parser = argparse.ArgumentParser(description="Start vLLM server")
       parser.add_argument("model_path", help="Path to model directory")
       parser.add_argument("--port", type=int, default=8000, help="Server port")
       parser.add_argument("--host", default="0.0.0.0", help="Server host")
       
       args = parser.parse_args()
       
       return start_vllm_server(args.model_path, args.port, args.host)
   
   if __name__ == "__main__":
       sys.exit(0 if main() else 1)
   EOF
   
   chmod +x /opt/citadel/scripts/start-vllm-server.py
   ```

2. **Create vLLM Client Test Script**
   ```bash
   # Create client test script
   tee /opt/citadel/scripts/test-vllm-client.py << 'EOF'
   #!/usr/bin/env python3
   """
   Test vLLM server with OpenAI-compatible client
   """
   
   import requests
   import json
   import time
   import argparse
   from rich.console import Console
   
   console = Console()
   
   def test_server_health(base_url):
       """Test server health endpoint"""
       try:
           response = requests.get(f"{base_url}/health", timeout=5)
           if response.status_code == 200:
               console.print("‚úÖ Server health check: PASSED")
               return True
           else:
               console.print(f"‚ùå Server health check failed: HTTP {response.status_code}")
               return False
       except Exception as e:
           console.print(f"‚ùå Server health check failed: {e}")
           return False
   
   def test_completion(base_url, model_name="test"):
       """Test completion endpoint"""
       try:
           url = f"{base_url}/v1/chat/completions"
           headers = {"Content-Type": "application/json"}
           
           payload = {
               "model": model_name,
               "messages": [
                   {"role": "user", "content": "Hello! Please respond with a short greeting."}
               ],
               "max_tokens": 50,
               "temperature": 0.7
           }
           
           console.print("üß™ Testing completion endpoint...")
           start_time = time.time()
           
           response = requests.post(url, headers=headers, json=payload, timeout=30)
           
           response_time = time.time() - start_time
           
           if response.status_code == 200:
               data = response.json()
               content = data["choices"][0]["message"]["content"]
               console.print(f"‚úÖ Completion test: PASSED ({response_time:.2f}s)")
               console.print(f"   Response: {content}")
               return True
           else:
               console.print(f"‚ùå Completion test failed: HTTP {response.status_code}")
               console.print(f"   Response: {response.text}")
               return False
               
       except Exception as e:
           console.print(f"‚ùå Completion test failed: {e}")
           return False
   
   def main():
       parser = argparse.ArgumentParser(description="Test vLLM server")
       parser.add_argument("--url", default="http://localhost:8000", help="Server URL")
       parser.add_argument("--model", default="test", help="Model name")
       
       args = parser.parse_args()
       
       console.print("üß™ vLLM Server Test")
       console.print("=" * 30)
       
       tests = [
           ("Health Check", lambda: test_server_health(args.url)),
           ("Completion", lambda: test_completion(args.url, args.model))
       ]
       
       passed = 0
       for test_name, test_func in tests:
           console.print(f"\nüìã Running {test_name}...")
           if test_func():
               passed += 1
       
       console.print(f"\nüìä Results: {passed}/{len(tests)} tests passed")
       return passed == len(tests)
   
   if __name__ == "__main__":
       sys.exit(0 if main() else 1)
   EOF
   
   chmod +x /opt/citadel/scripts/test-vllm-client.py
   ```

## Validation Steps

### Step 1: Installation Verification
```bash
# Comprehensive installation verification (Both Methods)
echo "=== vLLM Installation Verification ==="
source /opt/citadel/dev-env/bin/activate

# Check vLLM version and core functionality
python -c "
import vllm
import torch
import transformers
import fastapi

print('=== Installation Summary ===')
print(f'‚úÖ vLLM version: {vllm.__version__}')
print(f'‚úÖ PyTorch: {torch.__version__}')
print(f'‚úÖ Transformers: {transformers.__version__}')
print(f'‚úÖ FastAPI: {fastapi.__version__}')
print(f'‚úÖ CUDA available: {torch.cuda.is_available()}')
print(f'‚úÖ GPU count: {torch.cuda.device_count()}')

if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')
print('üéâ All core components verified!')
"
```

### Step 2: Functionality Testing
```bash
# Run comprehensive functionality test
echo "=== vLLM Functionality Testing ==="
source /opt/citadel/dev-env/bin/activate
python /opt/citadel/scripts/test-vllm.py
```

### Step 3: Performance Verification
```bash
# Basic performance verification
echo "=== vLLM Performance Verification ==="
source /opt/citadel/dev-env/bin/activate

python << 'EOF'
import torch
import time
from vllm import LLM, SamplingParams

if torch.cuda.is_available():
    print("Testing vLLM performance...")
    
    # Use a small model for performance testing
    try:
        llm = LLM(
            model="facebook/opt-125m",
            tensor_parallel_size=1,
            gpu_memory_utilization=0.3
        )
        
        # Performance test
        prompts = ["Hello world!"] * 10
        sampling_params = SamplingParams(max_tokens=20)
        
        start_time = time.time()
        outputs = llm.generate(prompts, sampling_params)
        end_time = time.time()
        
        total_time = end_time - start_time
        throughput = len(prompts) / total_time
        
        print(f"‚úÖ Performance test completed:")
        print(f"   Requests: {len(prompts)}")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   Throughput: {throughput:.2f} requests/second")
        
    except Exception as e:
        print(f"‚ùå Performance test failed: {e}")
else:
    print("‚ùå CUDA not available for performance testing")
EOF
```

## Troubleshooting

### Issue: vLLM Installation Fails
**Symptoms**: Compilation errors, dependency conflicts
**Solutions**:
- Check build dependencies: `sudo apt install build-essential cmake ninja-build`
- Update pip: `pip install --upgrade pip setuptools wheel`
- Try source installation: Follow alternative installation steps
- Check CUDA compatibility: Verify NVIDIA drivers and CUDA toolkit

### Issue: Import Errors
**Symptoms**: `ModuleNotFoundError`, import failures
**Solutions**:
- Verify environment: `source /opt/citadel/vllm-env/bin/activate`
- Check installation: `pip list | grep vllm`
- Reinstall: `pip uninstall vllm && pip install vllm`
- Check Python path: `python -c "import sys; print(sys.path)"`

### Issue: CUDA/GPU Not Detected
**Symptoms**: `torch.cuda.is_available()` returns False
**Solutions**:
- Verify NVIDIA drivers: `nvidia-smi`
- Check CUDA installation: `nvcc --version`
- Reinstall PyTorch: `pip install torch --index-url https://download.pytorch.org/whl/cu124`
- Check environment variables: `echo $CUDA_HOME`

### Issue: Memory Issues
**Symptoms**: OOM errors, allocation failures
**Solutions**:
- Reduce GPU memory utilization: `gpu_memory_utilization=0.7`
- Use smaller models for testing
- Check GPU memory: `nvidia-smi`
- Increase system swap if needed

## Configuration Summary

### Installed Components
- ‚úÖ **vLLM**: Latest compatible version (0.6.1+)
- ‚úÖ **PyTorch**: 2.4+ with CUDA 12.4 support
- ‚úÖ **Dependencies**: All required packages installed
- ‚úÖ **Flash Attention**: Optional performance enhancement
- ‚úÖ **Hugging Face**: CLI and authentication configured

### Testing Tools
- **test-vllm.py**: Comprehensive installation test
- **start-vllm-server.py**: Simple server startup script
- **test-vllm-client.py**: Client testing script

### Performance Optimizations
- Proper CUDA architecture targeting (8.9 for RTX 4070 Ti SUPER)
- Optimized compilation flags
- Flash Attention for improved performance
- Memory-efficient configuration

## Next Steps

Continue to **[PLANB-06-Storage-Symlinks.md](PLANB-06-Storage-Symlinks.md)** for model storage symlink configuration and integration with the dedicated storage setup.

---

**Task Status**: ‚ö†Ô∏è **Ready for Implementation**  
**Estimated Time**: 60-90 minutes  
**Complexity**: High  
**Prerequisites**: Python 3.12, PyTorch 2.4+, NVIDIA drivers, dedicated storage configured